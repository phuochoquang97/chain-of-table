{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment on Chain of Tables using Gemma2-9b-it model from groq \n",
    "\n",
    "Groq: https://groq.com/\n",
    "\n",
    "Paper: https://arxiv.org/abs/2401.04398\n",
    "\n",
    "##### Note: Groq provides API key for free, and some free usage, which has some limits such as token or requests per minute.  To get API get go to https://console.groq.com/keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import openai\n",
    "import time\n",
    "import glob \n",
    "from groq import Groq\n",
    "\n",
    "from utils.load_data import wrap_input_for_demo\n",
    "from utils.llm import ChatGPT\n",
    "from utils.helper import *\n",
    "from utils.evaluate import *\n",
    "from utils.chain import *\n",
    "from operations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User parameters\n",
    "model_name: str = \"gemma2-9b-it\"\n",
    "api_key: str = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_llm = ChatGPT(\n",
    "    model_name=model_name,\n",
    "    key=api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare WikiTQ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WikiTableQuestions is already downloaded\n"
     ]
    }
   ],
   "source": [
    "wiki_tq_dir = \"WikiTableQuestions/\"\n",
    "if os.path.exists(wiki_tq_dir) and os.path.isdir(wiki_tq_dir):\n",
    "    print(f\"WikiTableQuestions is already downloaded\")\n",
    "else:\n",
    "    # Step 1: Download the zip file\n",
    "    url = \"https://github.com/ppasupat/WikiTableQuestions/releases/download/v1.0.2/WikiTableQuestions-1.0.2-compact.zip\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Step 2: Unzip the contents\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "        zip_ref.extractall(\"WikiTableQuestions\")\n",
    "\n",
    "    print(\"Download and extraction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a subset of test datset, including 100 questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance</th>\n",
       "      <th>context</th>\n",
       "      <th>targetValue</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nt-2</th>\n",
       "      <td>which team won previous to crettyard?</td>\n",
       "      <td>csv/204-csv/772.csv</td>\n",
       "      <td>Wolfe Tones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-9</th>\n",
       "      <td>which players played the same position as ardo...</td>\n",
       "      <td>csv/203-csv/116.csv</td>\n",
       "      <td>Siim Ennemuist|Andri Aganits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-24</th>\n",
       "      <td>who ranked right after turkey?</td>\n",
       "      <td>csv/203-csv/812.csv</td>\n",
       "      <td>Sweden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-36</th>\n",
       "      <td>who was the top winner in 2002 of the division...</td>\n",
       "      <td>csv/204-csv/879.csv</td>\n",
       "      <td>Princeton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-42</th>\n",
       "      <td>what is the total number of popular votes cast...</td>\n",
       "      <td>csv/203-csv/558.csv</td>\n",
       "      <td>459,640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-43</th>\n",
       "      <td>which division three team also played in the d...</td>\n",
       "      <td>csv/202-csv/73.csv</td>\n",
       "      <td>Seaford Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-54</th>\n",
       "      <td>does theodis or david play center?</td>\n",
       "      <td>csv/204-csv/847.csv</td>\n",
       "      <td>Theodis Tarver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-72</th>\n",
       "      <td>what is the number of formula one series races...</td>\n",
       "      <td>csv/203-csv/198.csv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-75</th>\n",
       "      <td>how many places list no zip code in either the...</td>\n",
       "      <td>csv/204-csv/356.csv</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-80</th>\n",
       "      <td>has the dominican republic won more or less me...</td>\n",
       "      <td>csv/203-csv/535.csv</td>\n",
       "      <td>less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-81</th>\n",
       "      <td>what vehicle maker other than dodge has the mo...</td>\n",
       "      <td>csv/204-csv/89.csv</td>\n",
       "      <td>Chevrolet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-84</th>\n",
       "      <td>which year had the most titles released?</td>\n",
       "      <td>csv/204-csv/643.csv</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-85</th>\n",
       "      <td>name someone else from scotland inducted befor...</td>\n",
       "      <td>csv/204-csv/650.csv</td>\n",
       "      <td>George Burley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-86</th>\n",
       "      <td>what party has the most mp's?</td>\n",
       "      <td>csv/203-csv/139.csv</td>\n",
       "      <td>Serbian Progressive Party Српска напредна стра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-89</th>\n",
       "      <td>what was the number of days of the denver open?</td>\n",
       "      <td>csv/204-csv/536.csv</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-93</th>\n",
       "      <td>count how many of these members were unionists.</td>\n",
       "      <td>csv/204-csv/608.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-112</th>\n",
       "      <td>what is the first airbase listed on the chart?</td>\n",
       "      <td>csv/204-csv/102.csv</td>\n",
       "      <td>Abu al-Duhur Military Airbase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-120</th>\n",
       "      <td>which opponent has the most wins</td>\n",
       "      <td>csv/204-csv/836.csv</td>\n",
       "      <td>Bahrain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-121</th>\n",
       "      <td>what property comes before tensile elongation?</td>\n",
       "      <td>csv/204-csv/229.csv</td>\n",
       "      <td>Tensile Modulus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-122</th>\n",
       "      <td>the team's record in 2011 was the same was it'...</td>\n",
       "      <td>csv/204-csv/32.csv</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                utterance  \\\n",
       "id                                                          \n",
       "nt-2                which team won previous to crettyard?   \n",
       "nt-9    which players played the same position as ardo...   \n",
       "nt-24                      who ranked right after turkey?   \n",
       "nt-36   who was the top winner in 2002 of the division...   \n",
       "nt-42   what is the total number of popular votes cast...   \n",
       "nt-43   which division three team also played in the d...   \n",
       "nt-54                  does theodis or david play center?   \n",
       "nt-72   what is the number of formula one series races...   \n",
       "nt-75   how many places list no zip code in either the...   \n",
       "nt-80   has the dominican republic won more or less me...   \n",
       "nt-81   what vehicle maker other than dodge has the mo...   \n",
       "nt-84            which year had the most titles released?   \n",
       "nt-85   name someone else from scotland inducted befor...   \n",
       "nt-86                       what party has the most mp's?   \n",
       "nt-89     what was the number of days of the denver open?   \n",
       "nt-93     count how many of these members were unionists.   \n",
       "nt-112     what is the first airbase listed on the chart?   \n",
       "nt-120                   which opponent has the most wins   \n",
       "nt-121     what property comes before tensile elongation?   \n",
       "nt-122  the team's record in 2011 was the same was it'...   \n",
       "\n",
       "                    context                                        targetValue  \n",
       "id                                                                              \n",
       "nt-2    csv/204-csv/772.csv                                        Wolfe Tones  \n",
       "nt-9    csv/203-csv/116.csv                       Siim Ennemuist|Andri Aganits  \n",
       "nt-24   csv/203-csv/812.csv                                             Sweden  \n",
       "nt-36   csv/204-csv/879.csv                                          Princeton  \n",
       "nt-42   csv/203-csv/558.csv                                            459,640  \n",
       "nt-43    csv/202-csv/73.csv                                       Seaford Town  \n",
       "nt-54   csv/204-csv/847.csv                                     Theodis Tarver  \n",
       "nt-72   csv/203-csv/198.csv                                                  2  \n",
       "nt-75   csv/204-csv/356.csv                                                 18  \n",
       "nt-80   csv/203-csv/535.csv                                               less  \n",
       "nt-81    csv/204-csv/89.csv                                          Chevrolet  \n",
       "nt-84   csv/204-csv/643.csv                                               2005  \n",
       "nt-85   csv/204-csv/650.csv                                      George Burley  \n",
       "nt-86   csv/203-csv/139.csv  Serbian Progressive Party Српска напредна стра...  \n",
       "nt-89   csv/204-csv/536.csv                                                  5  \n",
       "nt-93   csv/204-csv/608.csv                                                  1  \n",
       "nt-112  csv/204-csv/102.csv                      Abu al-Duhur Military Airbase  \n",
       "nt-120  csv/204-csv/836.csv                                            Bahrain  \n",
       "nt-121  csv/204-csv/229.csv                                    Tensile Modulus  \n",
       "nt-122   csv/204-csv/32.csv                                               2009  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cases = pd.read_csv(wiki_tq_dir + \"data/random-split-4-dev.tsv\", sep=\"\\t\").head(100)\n",
    "test_cases = test_cases.set_index(\"id\")\n",
    "test_cases.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_table_text(df):\n",
    "    return [list(df.columns)] + df.astype(str).values.tolist()\n",
    "\n",
    "def normalize_answer(ans, normalize_numbers=True):\n",
    "    # Remove \"assistant:\" prefix\n",
    "    ans = re.sub(r'^Answer:\\s*', '', ans, flags=re.IGNORECASE)\n",
    "\n",
    "    # Lowercase\n",
    "    ans = ans.lower()\n",
    "\n",
    "    # Replace \" and \" with \"|\"\n",
    "    ans = ans.replace(\" and \", \"|\")\n",
    "\n",
    "    # Remove punctuation (except \"|\")\n",
    "    ans = re.sub(r\"[^\\w\\s|]\", \"\", ans)\n",
    "\n",
    "    # Normalize numbers by removing text after numeric values if needed\n",
    "    if normalize_numbers:\n",
    "        ans = re.sub(r'(\\d+)[^\\d|]*', r'\\1', ans)\n",
    "\n",
    "    # Remove extra spaces around delimiters\n",
    "    ans = \"|\".join(part.strip() for part in ans.split(\"|\"))\n",
    "\n",
    "    return ans.strip()\n",
    "\n",
    "\n",
    "\n",
    "def merge_all_results(csv_folder):\n",
    "    csv_files = glob.glob(os.path.join(csv_folder, \"results_batch_*.csv\"))\n",
    "    dfs = []\n",
    "\n",
    "    # Read and append each CSV file\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Merge all DataFrames\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    merged_csv_path = os.path.join(csv_folder, \"merged_results.csv\")\n",
    "    merged_df.to_csv(merged_csv_path, index=False)\n",
    "\n",
    "    print(f\"All CSV files merged successfully into '{merged_csv_path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiTQ With gemma2-9b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "test_count = 0\n",
    "results=[]\n",
    "result_dir: str = \"results/wiqiTQ\"\n",
    "\n",
    "\n",
    "# Ensure the result directory exists\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "for testcase_id in test_cases.index:\n",
    "    try:\n",
    "        file_path = os.path.join(result_dir, f\"{testcase_id}.pkl\")\n",
    "\n",
    "        # Skip test case if pkl file already exists\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Skipping {testcase_id} - already processed.\")\n",
    "            continue\n",
    "\n",
    "        df_path = wiki_tq_dir + test_cases.loc[testcase_id][\"context\"]\n",
    "        df = pd.read_csv(df_path)\n",
    "        statement = test_cases.loc[testcase_id][\"utterance\"]\n",
    "        answer = test_cases.loc[testcase_id][\"targetValue\"]\n",
    "        \n",
    "        table_caption = \"\"\n",
    "        table_text = convert_df_to_table_text(df)\n",
    "        \n",
    "        demo_sample = wrap_input_for_demo(\n",
    "        statement=statement, table_caption=table_caption, table_text=table_text\n",
    "        )\n",
    "        proc_sample, dynamic_chain_log = dynamic_chain_exec_one_sample(\n",
    "            sample=demo_sample, llm=gpt_llm\n",
    "        )\n",
    "        output_sample = simple_query(\n",
    "            sample=proc_sample,\n",
    "            table_info=get_table_info(proc_sample),\n",
    "            llm=gpt_llm,\n",
    "            use_demo=False,\n",
    "            llm_options=gpt_llm.get_model_options(\n",
    "                temperature=0.0, per_example_max_decode_steps=200, per_example_top_p=1.0\n",
    "            ),\n",
    "        )\n",
    "        cotable_log = get_table_log(output_sample)\n",
    "\n",
    "\n",
    "        # Save the log to a .pkl file\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(cotable_log, f)\n",
    "        \n",
    "        response = cotable_log[-1]['cotable_result']\n",
    "        response = response.strip().strip(\"'\\\"\")\n",
    "        final_response = response.split(\"Answer:\")[-1].strip()\n",
    "        \n",
    "        print(f\"ID: {testcase_id} | Response: {final_response} | Ground Truth: {answer}\")\n",
    "        results.append({'ID':testcase_id, 'Response':final_response, 'Ground Truth':answer})\n",
    "        if final_response == answer:\n",
    "            acc += 1\n",
    "        test_count += 1\n",
    "        if test_count % 5 == 0:\n",
    "            print(\"Sleeping for 1 minute to prevent rate limiting...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {testcase_id}\") \n",
    "        \n",
    "print(f\"number of correct response = {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Due to limitation on input tokens and requests per day, I have run above code using different api keys, so i saved csv files like batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to save processed data\n",
    "# df = pd.DataFrame(results)\n",
    "# df.to_csv(\"results_batch_5.csv\", index=False, encoding='utf-8')\n",
    "# print(\"CSV file saved successfully!\")\n",
    "\n",
    "# uncomment to merge all \n",
    "# merge_all_results('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Results for 100 tables.\n",
    "\n",
    "Note: Here we have results for 98 tables, becuase two table has incorrect csv format so it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no of rows : 98\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Response</th>\n",
       "      <th>Ground Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nt-2</td>\n",
       "      <td>Confey</td>\n",
       "      <td>Wolfe Tones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nt-9</td>\n",
       "      <td>Siim Ennemuist</td>\n",
       "      <td>Siim Ennemuist|Andri Aganits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nt-24</td>\n",
       "      <td>I need more information to answer this question.</td>\n",
       "      <td>Sweden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nt-36</td>\n",
       "      <td>Princeton</td>\n",
       "      <td>Princeton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nt-42</td>\n",
       "      <td>459,640</td>\n",
       "      <td>459,640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nt-43</td>\n",
       "      <td>East Preston</td>\n",
       "      <td>Seaford Town</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nt-54</td>\n",
       "      <td>Theodis Tarver plays center.</td>\n",
       "      <td>Theodis Tarver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nt-72</td>\n",
       "      <td>Test driver</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nt-75</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nt-80</td>\n",
       "      <td>The Dominican Republic has won more medals tha...</td>\n",
       "      <td>less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nt-81</td>\n",
       "      <td>Chevrolet</td>\n",
       "      <td>Chevrolet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nt-84</td>\n",
       "      <td>2005</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nt-85</td>\n",
       "      <td>George Burley*</td>\n",
       "      <td>George Burley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nt-86</td>\n",
       "      <td>Serbian Progressive Party</td>\n",
       "      <td>Serbian Progressive Party Српска напредна стра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nt-89</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nt-93</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nt-112</td>\n",
       "      <td>Abu al-Duhur Military Airbase</td>\n",
       "      <td>Abu al-Duhur Military Airbase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>nt-120</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>Bahrain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nt-121</td>\n",
       "      <td>Tensile Modulus</td>\n",
       "      <td>Tensile Modulus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>nt-122</td>\n",
       "      <td>2007</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                           Response  \\\n",
       "0     nt-2                                             Confey   \n",
       "1     nt-9                                     Siim Ennemuist   \n",
       "2    nt-24   I need more information to answer this question.   \n",
       "3    nt-36                                          Princeton   \n",
       "4    nt-42                                            459,640   \n",
       "5    nt-43                                       East Preston   \n",
       "6    nt-54                       Theodis Tarver plays center.   \n",
       "7    nt-72                                        Test driver   \n",
       "8    nt-75                                                 18   \n",
       "9    nt-80  The Dominican Republic has won more medals tha...   \n",
       "10   nt-81                                          Chevrolet   \n",
       "11   nt-84                                               2005   \n",
       "12   nt-85                                     George Burley*   \n",
       "13   nt-86                          Serbian Progressive Party   \n",
       "14   nt-89                                                  5   \n",
       "15   nt-93                                                  1   \n",
       "16  nt-112                      Abu al-Duhur Military Airbase   \n",
       "17  nt-120                                        South Korea   \n",
       "18  nt-121                                    Tensile Modulus   \n",
       "19  nt-122                                               2007   \n",
       "\n",
       "                                         Ground Truth  \n",
       "0                                         Wolfe Tones  \n",
       "1                        Siim Ennemuist|Andri Aganits  \n",
       "2                                              Sweden  \n",
       "3                                           Princeton  \n",
       "4                                             459,640  \n",
       "5                                        Seaford Town  \n",
       "6                                      Theodis Tarver  \n",
       "7                                                   2  \n",
       "8                                                  18  \n",
       "9                                                less  \n",
       "10                                          Chevrolet  \n",
       "11                                               2005  \n",
       "12                                      George Burley  \n",
       "13  Serbian Progressive Party Српска напредна стра...  \n",
       "14                                                  5  \n",
       "15                                                  1  \n",
       "16                      Abu al-Duhur Military Airbase  \n",
       "17                                            Bahrain  \n",
       "18                                    Tensile Modulus  \n",
       "19                                               2009  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_df= pd.read_csv('./merged_results.csv')\n",
    "print(f\"Total no of rows : {all_results_df.shape[0]}\")\n",
    "all_results_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize results and get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: nt-2, Expected: wolfe tones, Predicted: confey \n",
      "ID: nt-9, Expected: siim ennemuist|andri aganits, Predicted: siim ennemuist \n",
      "ID: nt-24, Expected: sweden, Predicted: i need more information to answer this question \n",
      "ID: nt-43, Expected: seaford town, Predicted: east preston \n",
      "ID: nt-54, Expected: theodis tarver, Predicted: theodis tarver plays center \n",
      "ID: nt-72, Expected: 2, Predicted: test driver \n",
      "ID: nt-80, Expected: less, Predicted: the dominican republic has won more medals than china \n",
      "ID: nt-86, Expected: serbian progressive party српска напредна странка  srpska napredna stranka, Predicted: serbian progressive party \n",
      "ID: nt-120, Expected: bahrain, Predicted: south korea \n",
      "ID: nt-122, Expected: 2009, Predicted: 2007 \n",
      "ID: nt-123, Expected: tikamgarh, Predicted: chhatarpur \n",
      "ID: nt-138, Expected: switzerland, Predicted: france \n",
      "ID: nt-141, Expected: 18, Predicted: this table does not contain information about games played by senators \n",
      "ID: nt-153, Expected: 10, Predicted: 100 \n",
      "ID: nt-154, Expected: pan troglodytes|nomascus leucogenys, Predicted: chimpanzee|gibbon \n",
      "ID: nt-171, Expected: 2, Predicted: 9 \n",
      "ID: nt-174, Expected: 26, Predicted: 46 \n",
      "ID: nt-182, Expected: 10, Predicted: 9 \n",
      "ID: nt-186, Expected: 3, Predicted: 9 \n",
      "ID: nt-192, Expected: 4, Predicted: 7 \n",
      "ID: nt-230, Expected: iivo niskanen|daniel richardsson|johan olsson|dario cologna, Predicted: dario cologna johan olsson daniel richardsson iivo niskanen \n",
      "ID: nt-234, Expected: tianhe stadium guangzhou, Predicted: i need more information to answer this question \n",
      "ID: nt-239, Expected: 2010, Predicted: 2010|2012 \n",
      "ID: nt-242, Expected: re 44, Predicted: ad43 \n",
      "ID: nt-244, Expected: 5, Predicted: 3 \n",
      "ID: nt-258, Expected: 3, Predicted: 2 \n",
      "ID: nt-279, Expected: 11, Predicted: 6 \n",
      "ID: nt-285, Expected: jeanphilippe ruggia, Predicted: i cannot answer this question \n",
      "ID: nt-296, Expected: world championships, Predicted: hypomeeting \n",
      "ID: nt-301, Expected: 115, Predicted: this table does not contain information about player nationalities \n",
      "ID: nt-305, Expected: czechoslovakia, Predicted: this table does not contain information about which country was the first to sell weapons to iraq \n",
      "ID: nt-306, Expected: more, Predicted: chicago won more \n",
      "ID: nt-309, Expected: european u23, Predicted: shot put \n",
      "ID: nt-335, Expected: borussia dortmund|bayern munich, Predicted: bayern munich borussia dortmund \n",
      "ID: nt-365, Expected: henry e prickett, Predicted: thomas b hart \n",
      "ID: nt-380, Expected: no, Predicted: cannot be determined from the table \n",
      "ID: nt-390, Expected: 62, Predicted: 18801942 \n",
      "ID: nt-396, Expected: chipola, Predicted: valencia cc|chipola \n",
      "ID: nt-405, Expected: 7, Predicted: 12 \n",
      "ID: nt-410, Expected: anime friends, Predicted: anime expo \n",
      "ID: nt-417, Expected: 7, Predicted: the table does not contain information about sarah churchill \n",
      "ID: nt-432, Expected: 20301261, Predicted: 2030 \n",
      "ID: nt-445, Expected: 2013, Predicted: 20130 \n",
      "ID: nt-472, Expected: nc state, Predicted: 1 \n",
      "ID: nt-486, Expected: 9997, Predicted: 10000 \n",
      "ID: nt-500, Expected: ireland, Predicted: irl \n",
      "----------------------------------------\n",
      "Accuracy: 53.06%\n"
     ]
    }
   ],
   "source": [
    "accuracy=0\n",
    "incorrect_ids=[]\n",
    "for index, row in all_results_df.iterrows():\n",
    "    y_pred= normalize_answer(row['Response'])\n",
    "    y_true= normalize_answer(row['Ground Truth'])\n",
    "    if y_pred == y_true:\n",
    "        accuracy+=1\n",
    "    else:\n",
    "        print(f\"ID: {row['ID']}, Expected: {y_true}, Predicted: {y_pred} \")\n",
    "        incorrect_ids.append(row['ID'])\n",
    "\n",
    "print(\"-\"*40)\n",
    "accuracy_percentage = (accuracy / all_results_df.shape[0]) * 100\n",
    "print(f\"Accuracy: {accuracy_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: By verifying the failures, there are 7 more correct answers, dues to larger or different format of prompts: so the the accuracy using WikiTQ dataset with model gemma2, using chain of tables is 60%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine some Failed testcases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example with chain-of-table\n",
    "def get_chain_of_table(table_text, wrap_input, llm, answer):\n",
    "    \n",
    "    proc_sample, dynamic_chain_log = dynamic_chain_exec_one_sample(\n",
    "        sample=wrap_input, llm=llm\n",
    "    )\n",
    "    output_sample = simple_query(\n",
    "        sample=proc_sample,\n",
    "        table_info=get_table_info(proc_sample),\n",
    "        llm=llm,\n",
    "        use_demo=False,\n",
    "        llm_options=llm.get_model_options(\n",
    "            temperature=0.0, per_example_max_decode_steps=200, per_example_top_p=1.0\n",
    "        ),\n",
    "    )\n",
    "    cotable_log = get_table_log(output_sample)\n",
    "    \n",
    "    print(f'Question: {output_sample[\"statement\"]}\\n')\n",
    "    print(f'Table: {output_sample[\"table_caption\"]}')\n",
    "    print(f\"{pd.DataFrame(table_text[1:], columns=table_text[0])}\\n\")\n",
    "    \n",
    "    for table_info in cotable_log:\n",
    "        if table_info[\"act_chain\"]:\n",
    "            table_text = table_info[\"table_text\"]\n",
    "            table_action = table_info[\"act_chain\"][-1]\n",
    "            if \"skip\" in table_action:\n",
    "                continue\n",
    "            if \"query\" in table_action:\n",
    "                result = table_info[\"cotable_result\"]\n",
    "                print(f\"{result}\")\n",
    "            else:\n",
    "                print(f\"-> {table_action}\\n{pd.DataFrame(table_text[1:], columns=table_text[0])}\")\n",
    "                if 'group_sub_table' in table_info:\n",
    "                    group_column, group_info = table_info[\"group_sub_table\"]\n",
    "                    group_headers = [\"Group ID\", group_column, \"Count\"]\n",
    "                    group_rows = []\n",
    "                    for i, (v, count) in enumerate(group_info):\n",
    "                        if v.strip() == \"\":\n",
    "                            v = \"[Empty Cell]\"\n",
    "                        group_rows.append([f\"Group {i+1}\", v, str(count)])\n",
    "                    print(f\"{pd.DataFrame(group_rows, columns=group_headers)}\")\n",
    "                print()\n",
    "\n",
    "    print(f\"Groundtruth: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testcase nt-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase_id = \"nt-9\"\n",
    "df_path = wiki_tq_dir + test_cases.loc[testcase_id][\"context\"]\n",
    "statement = test_cases.loc[testcase_id][\"utterance\"]\n",
    "answer = test_cases.loc[testcase_id][\"targetValue\"] \n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "table_caption = \"\"\n",
    "table_text = convert_df_to_table_text(df)\n",
    "\n",
    "demo_sample = wrap_input_for_demo(\n",
    "    statement=statement, table_caption=table_caption, table_text=table_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: which players played the same position as ardo kreek?\n",
      "\n",
      "Table: \n",
      "   No.           Player                  Birth Date Weight Height  \\\n",
      "0    4       Ardo Kreek     August 7, 1986 (age 27)     96    203   \n",
      "1    5      Kert Toobal       June 3, 1979 (age 35)     78    189   \n",
      "2    6   Martti Juhkami       June 6, 1988 (age 26)     96    196   \n",
      "3    7    Argo Meresaar   January 13, 1980 (age 34)    107    206   \n",
      "4    8     Kusti Nõlvak   November 6, 1991 (age 22)     81    186   \n",
      "5    9      Robert Täht    August 15, 1993 (age 20)     80    190   \n",
      "6   11     Oliver Venno       May 23, 1990 (age 24)    105    210   \n",
      "7   14     Rait Rikberg    August 30, 1982 (age 31)     80    174   \n",
      "8   16  Edgar Järvekülg      June 12, 1988 (age 26)     77    186   \n",
      "9   17   Siim Ennemuist   December 5, 1989 (age 24)     89    196   \n",
      "10  18  Jaanus Nõmmsalu   January 19, 1981 (age 33)     94    200   \n",
      "11  19    Andri Aganits  September 7, 1993 (age 20)     99    207   \n",
      "\n",
      "          Position      Current Club  \n",
      "0   Middle blocker      Paris Volley  \n",
      "1           Setter     Sivas 4 Eylül  \n",
      "2           Spiker           TV Bühl  \n",
      "3         Opposite     Bigbank Tartu  \n",
      "4           Setter            TTÜ VK  \n",
      "5           Spiker     Bigbank Tartu  \n",
      "6         Opposite  Rennes Volley 35  \n",
      "7           Libero     Bigbank Tartu  \n",
      "8           Libero          Pärnu VK  \n",
      "9   Middle blocker            TTÜ VK  \n",
      "10          Spiker            TTÜ VK  \n",
      "11  Middle Blocker           TV Bühl  \n",
      "\n",
      "-> f_select_row(row 9)\n",
      "  No.          Player                 Birth Date Weight Height  \\\n",
      "0  17  Siim Ennemuist  December 5, 1989 (age 24)     89    196   \n",
      "\n",
      "         Position Current Club  \n",
      "0  Middle blocker       TTÜ VK  \n",
      "\n",
      "-> f_select_column(*)\n",
      "  No.          Player                 Birth Date Weight Height  \\\n",
      "0  17  Siim Ennemuist  December 5, 1989 (age 24)     89    196   \n",
      "\n",
      "         Position Current Club  \n",
      "0  Middle blocker       TTÜ VK  \n",
      "\n",
      "Answer:  Siim Ennemuist\n",
      "Groundtruth: Siim Ennemuist|Andri Aganits\n"
     ]
    }
   ],
   "source": [
    "get_chain_of_table(table_text, demo_sample, gpt_llm, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: The model selected the first row where position is same, instead of selecting all rows where position is same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test with direct prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Serialize the table (you can adjust formatting)\n",
    "def serialize_table(df):\n",
    "    return df.to_csv(index=False)\n",
    "\n",
    "# Build the prompt\n",
    "def build_prompt(df, question):\n",
    "    serialized_table = serialize_table(df)\n",
    "    prompt_str = f\"\"\"\\\n",
    "Here's a serialized table.\n",
    "\n",
    "{serialized_table}\n",
    "\n",
    "Please answer the question: {question}\n",
    "Answer: \"\"\"\n",
    "    return prompt_str\n",
    "\n",
    "# Query Groq API\n",
    "def query_groq(prompt):\n",
    "    client = Groq(api_key=api_key)\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gemma2-9b-it\",\n",
    "        messages=prompt,\n",
    "        temperature=0,\n",
    "        max_completion_tokens=1024,\n",
    "        top_p=1,\n",
    "        stream=True,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    response = \"\"  # Store the response in a variable\n",
    "    for chunk in completion:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            response += chunk.choices[0].delta.content\n",
    "\n",
    "    return response.strip()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Andri Aganits \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt(df, statement)\n",
    "prompt_message=np.array([{\"role\": \"assistant\",\n",
    "                 \"content\": prompt}])\n",
    "query_groq(prompt_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: Through direct prompt model also have selected one correct name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testcase nt-141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is the number of games the senators have played?\n",
      "\n",
      "Table: \n",
      "     #         Date             Visitor Score                Home Record Pts\n",
      "0    1  December 21     Ottawa Senators   5–2  Montreal Canadiens  1–0–0   2\n",
      "1    2  December 26      Toronto Arenas   2–5     Ottawa Senators  2–0–0   4\n",
      "2    3  December 31     Ottawa Senators   2–4      Toronto Arenas  2–1–0   4\n",
      "3    4    January 2  Montreal Canadiens   2–7     Ottawa Senators  3–1–0   6\n",
      "4    5    January 4     Ottawa Senators   2–5  Montreal Canadiens  3–2–0   6\n",
      "5    6    January 9      Toronto Arenas   2–4     Ottawa Senators  4–2–0   8\n",
      "6    7   January 14     Ottawa Senators   2–5      Toronto Arenas  4–3–0   8\n",
      "7    8   January 16  Montreal Canadiens  10–6     Ottawa Senators  4–4–0   8\n",
      "8    9   January 18     Ottawa Senators   3–5  Montreal Canadiens  4–5–0   8\n",
      "9   10   January 23      Toronto Arenas   2–3     Ottawa Senators  5–5–0  10\n",
      "10  11   January 25     Ottawa Senators   1–0  Montreal Canadiens  1–0–0   2\n",
      "11  12   January 28     Ottawa Senators   2–1      Toronto Arenas  2–0–0   4\n",
      "12  13   January 30  Montreal Canadiens   2–3     Ottawa Senators  3–0–0   6\n",
      "13  14   February 6      Toronto Arenas   1–3     Ottawa Senators  4–0–0   8\n",
      "14  15   February 8     Ottawa Senators   3–4  Montreal Canadiens  4–1–0  18\n",
      "15  16  February 13  Montreal Canadiens   0–7     Ottawa Senators  5–1–0  10\n",
      "16  17  February 18     Ottawa Senators   4–3      Toronto Arenas  6–1–0  12\n",
      "17  18  February 20      Toronto Arenas   3–9     Ottawa Senators  7–1–0  14\n",
      "\n",
      "-> f_select_column(#)\n",
      "     #\n",
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    5\n",
      "5    6\n",
      "6    7\n",
      "7    8\n",
      "8    9\n",
      "9   10\n",
      "10  11\n",
      "11  12\n",
      "12  13\n",
      "13  14\n",
      "14  15\n",
      "15  16\n",
      "16  17\n",
      "17  18\n",
      "\n",
      "Answer:  This table does not contain information about games played by senators.\n",
      "Groundtruth: 18\n"
     ]
    }
   ],
   "source": [
    "testcase_id = \"nt-141\"\n",
    "df_path = wiki_tq_dir + test_cases.loc[testcase_id][\"context\"]\n",
    "statement = test_cases.loc[testcase_id][\"utterance\"]\n",
    "answer = test_cases.loc[testcase_id][\"targetValue\"]\n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "table_caption = \"\"\n",
    "table_text = convert_df_to_table_text(df)\n",
    "\n",
    "demo_sample = wrap_input_for_demo(\n",
    "    statement=statement, table_caption=table_caption, table_text=table_text\n",
    ")\n",
    "\n",
    "get_chain_of_table(table_text, demo_sample, gpt_llm, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test with direct prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt(df, statement)\n",
    "prompt_message=np.array([{\"role\": \"assistant\",\n",
    "                 \"content\": prompt}])\n",
    "query_groq(prompt_message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: The incorrect answer was given by chain of tables, but got correct answer through direct prompt. Reason could be in chain of tables the mdoel has selected the '#' columns, which is just index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test n-80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: has the dominican republic won more or less medals than china?\n",
      "\n",
      "Table: \n",
      "   Rank                 Nation Gold Silver Bronze Total\n",
      "0    1.          United States    8      7      1    16\n",
      "1    2.                 Russia    7      7      5    19\n",
      "2    3.                 France    3      3      2     8\n",
      "3    4.               Ethiopia    3      2      2     7\n",
      "4    5.                Belarus    3      1      3     7\n",
      "5    6.                 Sweden    2      1      2     5\n",
      "6    7.                  Kenya    2      1      1     4\n",
      "7    7=           South Africa    2      1      1     4\n",
      "8    9.                Morocco    2      1      0     3\n",
      "9   10.                 Greece    1      1      2     4\n",
      "10  11.                   Cuba    1      1      0     2\n",
      "11  12.                  Italy    1      0      2     3\n",
      "12  13.                 Canada    1      0      1     2\n",
      "13  14.                Algeria    1      0      0     1\n",
      "14  14=              Australia    1      0      0     1\n",
      "15  14=     Dominican Republic    1      0      0     1\n",
      "16  14=                Ecuador    1      0      0     1\n",
      "17  14=              Lithuania    1      0      0     1\n",
      "18  14=                 Mexico    1      0      0     1\n",
      "19  14=             Mozambique    1      0      0     1\n",
      "20  14=                 Poland    1      0      0     1\n",
      "21  14=                  Qatar    1      0      0     1\n",
      "22  14=  Saint Kitts and Nevis    1      0      0     1\n",
      "23  24.                Jamaica    0      4      2     6\n",
      "24  25.                  Spain    0      3      2     5\n",
      "25  26.                Hungary    0      2      0     2\n",
      "26  27.                Germany    0      1      3     4\n",
      "27  27=                  Japan    0      1      3     4\n",
      "28  27=                Ukraine    0      1      3     4\n",
      "29  30.          Great Britain    0      1      2     3\n",
      "30  31.                 Brazil    0      1      0     1\n",
      "31  31=               Cameroon    0      1      0     1\n",
      "32  31=         Czech Republic    0      1      0     1\n",
      "33  31=                Estonia    0      1      0     1\n",
      "34  31=                Ireland    0      1      0     1\n",
      "35  31=    Trinidad and Tobago    0      1      0     1\n",
      "36  31=                 Turkey    0      1      0     1\n",
      "37  38.                Bahamas    0      0      3     3\n",
      "38  39.                  China    0      0      2     2\n",
      "39  40.                  India    0      0      1     1\n",
      "40  40=             Kazakhstan    0      0      1     1\n",
      "41  40=            Netherlands    0      0      1     1\n",
      "42  40=                Senegal    0      0      1     1\n",
      "\n",
      "-> f_select_row(row 15, row 38)\n",
      "  Rank              Nation Gold Silver Bronze Total\n",
      "0  14=  Dominican Republic    1      0      0     1\n",
      "1  39.               China    0      0      2     2\n",
      "\n",
      "-> f_select_column(*)\n",
      "  Rank              Nation Gold Silver Bronze Total\n",
      "0  14=  Dominican Republic    1      0      0     1\n",
      "1  39.               China    0      0      2     2\n",
      "\n",
      "Answer: The Dominican Republic has won more medals than China.\n",
      "Groundtruth: less\n"
     ]
    }
   ],
   "source": [
    "testcase_id = \"nt-80\"\n",
    "df_path = wiki_tq_dir + test_cases.loc[testcase_id][\"context\"]\n",
    "statement = test_cases.loc[testcase_id][\"utterance\"]\n",
    "answer = test_cases.loc[testcase_id][\"targetValue\"] \n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "table_caption = \"\"\n",
    "table_text = convert_df_to_table_text(df)\n",
    "\n",
    "demo_sample = wrap_input_for_demo(\n",
    "    statement=statement, table_caption=table_caption, table_text=table_text\n",
    ")\n",
    "\n",
    "get_chain_of_table(table_text, demo_sample, gpt_llm, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test With direct prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Dominican Republic has won 1 medal. China has won 2 medals. \n",
      "\n",
      "Therefore, the Dominican Republic has won **less** medals than China.\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt(df, statement)\n",
    "prompt_message=np.array([{\"role\": \"assistant\",\n",
    "                 \"content\": prompt}])\n",
    "query_groq(prompt_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: Chain of tables gives wrong answer, while direct prompt gives the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test  nt-122\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: the team's record in 2011 was the same was it's record in what year\n",
      "\n",
      "Table: \n",
      "           Season   Division    W–L Finish   Home   Road     GF     GA  \\\n",
      "0            2006    Western   1–15    6th    0–8    1–7    150    202   \n",
      "1            2007    Western   6–10    5th    4–4    2–6    160    189   \n",
      "2            2008    Western   4–12    5th    3–5    1–7    141    197   \n",
      "3            2009    Western   5–11    6th    4–4    1–7    159    200   \n",
      "4            2010    Western   10–6    3rd    5–3    5–3    186    201   \n",
      "5            2011    Western   5–11    5th    4–4    1–7    175    204   \n",
      "6            2012    Western   6–10    4th    4–4    2–6    167    175   \n",
      "7            2013    Western    9–7    3rd    2–6    7–1    203    170   \n",
      "8           Total  8 seasons  46–82    nan  26–38  20–44  1,341  1,538   \n",
      "9  Playoff Totals        nan    3–3    nan    0–0    3–3     73     54   \n",
      "\n",
      "                              Coach                    Playoffs Avg Attendance  \n",
      "0                          Paul Day             Missed playoffs         10,367  \n",
      "1                          Paul Day             Missed playoffs         10,815  \n",
      "2  Paul Day (0–5)\\nBob Hamley (4–7)             Missed playoffs          8,820  \n",
      "3                        Bob Hamley             Missed playoffs          8,347  \n",
      "4                      Derek Keenan       Lost in Western Final          7,558  \n",
      "5                      Derek Keenan             Missed Playoffs          7,151  \n",
      "6                      Derek Keenan               Lost in Final          7,050  \n",
      "7                      Derek Keenan  Lost in Western Semi-final          6,714  \n",
      "8                               nan                         nan          8,353  \n",
      "9                               nan                         nan            nan  \n",
      "\n",
      "-> f_select_row(row 1, row 5)\n",
      "  Season Division   W–L Finish Home Road   GF   GA         Coach  \\\n",
      "0   2007  Western  6–10    5th  4–4  2–6  160  189      Paul Day   \n",
      "1   2011  Western  5–11    5th  4–4  1–7  175  204  Derek Keenan   \n",
      "\n",
      "          Playoffs Avg Attendance  \n",
      "0  Missed playoffs         10,815  \n",
      "1  Missed Playoffs          7,151  \n",
      "\n",
      "-> f_select_column(*)\n",
      "  Season Division   W–L Finish Home Road   GF   GA         Coach  \\\n",
      "0   2007  Western  6–10    5th  4–4  2–6  160  189      Paul Day   \n",
      "1   2011  Western  5–11    5th  4–4  1–7  175  204  Derek Keenan   \n",
      "\n",
      "          Playoffs Avg Attendance  \n",
      "0  Missed playoffs         10,815  \n",
      "1  Missed Playoffs          7,151  \n",
      "\n",
      "Answer: 2007\n",
      "Groundtruth: 2009\n"
     ]
    }
   ],
   "source": [
    "testcase_id = \"nt-122\"\n",
    "df_path = wiki_tq_dir + test_cases.loc[testcase_id][\"context\"]\n",
    "statement = test_cases.loc[testcase_id][\"utterance\"]\n",
    "answer = test_cases.loc[testcase_id][\"targetValue\"] \n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "table_caption = \"\"\n",
    "table_text = convert_df_to_table_text(df)\n",
    "\n",
    "demo_sample = wrap_input_for_demo(\n",
    "    statement=statement, table_caption=table_caption, table_text=table_text\n",
    ")\n",
    "\n",
    "get_chain_of_table(table_text, demo_sample, gpt_llm, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with direct prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt(df, statement)\n",
    "prompt_message=np.array([{\"role\": \"assistant\",\n",
    "                 \"content\": prompt}])\n",
    "query_groq(prompt_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: Here again we get the correct answer with direct prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine some Success testcases through direct prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case nt-36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth is: Princeton\n",
      "\n",
      "Princeton\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testcase_id = \"nt-36\"\n",
    "df_path = wiki_tq_dir + test_cases.loc[testcase_id][\"context\"]\n",
    "statement = test_cases.loc[testcase_id][\"utterance\"]\n",
    "answer = test_cases.loc[testcase_id][\"targetValue\"]\n",
    "df = pd.read_csv(df_path)\n",
    "print(f\"Ground truth is: {answer}\")\n",
    "prompt = build_prompt(df, statement)\n",
    "prompt_message=np.array([{\"role\": \"assistant\",\n",
    "                 \"content\": prompt}])\n",
    "query_groq(prompt_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Case nt-121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth is: Tensile Modulus\n",
      "\n",
      "Tensile Modulus\n"
     ]
    }
   ],
   "source": [
    "testcase_id = \"nt-121\"\n",
    "df_path = wiki_tq_dir + test_cases.loc[testcase_id][\"context\"]\n",
    "statement = test_cases.loc[testcase_id][\"utterance\"]\n",
    "answer = test_cases.loc[testcase_id][\"targetValue\"]\n",
    "df = pd.read_csv(df_path)\n",
    "print(f\"Ground truth is: {answer}\")\n",
    "prompt = build_prompt(df, statement)\n",
    "prompt_message=np.array([{\"role\": \"assistant\",\n",
    "                 \"content\": prompt}])\n",
    "query_groq(prompt_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since promopting is giving better results on some cases, Let's run the model for 100 test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: nt-2 | Response: Wolfe Tones | Ground Truth: Wolfe Tones\n",
      "ID: nt-9 | Response: Andri Aganits | Ground Truth: Siim Ennemuist|Andri Aganits\n",
      "ID: nt-24 | Response: Sweden | Ground Truth: Sweden\n",
      "ID: nt-36 | Response: Princeton | Ground Truth: Princeton\n",
      "ID: nt-42 | Response: 459,640 | Ground Truth: 459,640\n",
      "ID: nt-43 | Response: East Preston | Ground Truth: Seaford Town\n",
      "ID: nt-54 | Response: Theodis Tarver plays center. | Ground Truth: Theodis Tarver\n",
      "ID: nt-72 | Response: 0 | Ground Truth: 2\n",
      "ID: nt-75 | Response: 10 | Ground Truth: 18\n",
      "ID: nt-80 | Response: The Dominican Republic has won 1 medal. China has won 2 medals. \n",
      "\n",
      "Therefore, the Dominican Republic has won **less** medals than China. | Ground Truth: less\n",
      "ID: nt-81 | Response: Chevrolet has the most vehicles in the roster other than Dodge. | Ground Truth: Chevrolet\n",
      "ID: nt-84 | Response: 2005 | Ground Truth: 2005\n",
      "ID: nt-85 | Response: John Wark | Ground Truth: George Burley\n",
      "ID: nt-86 | Response: The Serbian Progressive Party has the most MPs with 134. | Ground Truth: Serbian Progressive Party Српска напредна странка / Srpska napredna stranka\n",
      "ID: nt-89 | Response: 5 | Ground Truth: 5\n",
      "ID: nt-93 | Response: 1 | Ground Truth: 1\n",
      "ID: nt-112 | Response: Abu al-Duhur Military Airbase | Ground Truth: Abu al-Duhur Military Airbase\n",
      "ID: nt-120 | Response: South Korea has the most wins against Bahrain. | Ground Truth: Bahrain\n",
      "ID: nt-121 | Response: Tensile Modulus | Ground Truth: Tensile Modulus\n",
      "ID: nt-122 | Response: 2009 | Ground Truth: 2009\n",
      "ID: nt-123 | Response: Tikamgarh | Ground Truth: Tikamgarh\n",
      "ID: nt-129 | Response: 2003 | Ground Truth: 2003\n",
      "ID: nt-132 | Response: 47 | Ground Truth: 47\n",
      "ID: nt-134 | Response: N | Ground Truth: N\n",
      "ID: nt-138 | Response:  | Ground Truth: Switzerland\n",
      "ID: nt-141 | Response: 18 | Ground Truth: 18\n",
      "ID: nt-153 | Response: 10.0 | Ground Truth: 10\n",
      "ID: nt-154 | Response: Chimpanzee and Gibbon. | Ground Truth: Pan troglodytes|Nomascus leucogenys\n",
      "ID: nt-163 | Response: Vokhid Shodiev | Ground Truth: Vokhid Shodiev\n",
      "ID: nt-167 | Response: BL26 | Ground Truth: BL26\n",
      "ID: nt-168 | Response: 1995 | Ground Truth: 1995\n",
      "ID: nt-171 | Response: 7 | Ground Truth: 2\n",
      "ID: nt-174 | Response: 30 | Ground Truth: 26\n",
      "ID: nt-182 | Response: 9 | Ground Truth: 10\n",
      "ID: nt-186 | Response: 3 | Ground Truth: 3\n",
      "ID: nt-192 | Response: 5 | Ground Truth: 4\n",
      "ID: nt-200 | Response: 30 | Ground Truth: 30\n",
      "ID: nt-201 | Response: Park and Missoula have a larger combined population. | Ground Truth: Park and Missoula\n",
      "ID: nt-205 | Response: 50 | Ground Truth: 5\n",
      "ID: nt-223 | Response: 1998 | Ground Truth: 1998\n",
      "ID: nt-226 | Response: December 21 | Ground Truth: December 21\n",
      "ID: nt-228 | Response: 39 | Ground Truth: 39\n",
      "ID: nt-229 | Response: T.V. Sivaraopantulu held the position longer. | Ground Truth: T.V. SivaraoPantulu\n",
      "ID: nt-230 | Response: Here are the players who finished ahead of Lukáš Bauer, based on the provided table:\n",
      "\n",
      "* **Dario Cologna** (Rank 35)\n",
      "* **Johan Olsson** (Rank 34)\n",
      "* **Daniel Richardsson** (Rank 43)\n",
      "* **Iivo Niskanen** (Rank 4)\n",
      "* **Chris Jespersen** (Rank 6)\n",
      "* **Alexander Bessmertnykh** (Rank 7)\n",
      "* **Axel Teichmann** (Rank 8)\n",
      "* **Alexey Poltoranin** (Rank 9)\n",
      "* **Marcus Hellner** (Rank 10)\n",
      "* **Hannes Dotzler** (Rank 11)\n",
      "* **Eldar Rønning** (Rank 12)\n",
      "* **Martin Johnsrud Sundby** (Rank 13)\n",
      "* **Jens Filbrich** (Rank 14)\n",
      "* **Lars Nelson** (Rank 15)\n",
      "* **Dmitry Japarov** (Rank 16)\n",
      "* **Sami Jauhojärvi** (Rank 17)\n",
      "* **Pål Golberg** (Rank 18)\n",
      "* **Stanislav Volzhentsev** (Rank 19)\n",
      "* **Matti Heikkinen** (Rank 20)\n",
      "* **Jean-Marc Gaillard** (Rank 21)\n",
      "* **Curdin Perl** (Rank 22)\n",
      "* **Martin Bajčičák** (Rank 23)\n",
      "* **Jonas Baumann** (Rank 24)\n",
      "* **Evgeniy Belov** (Rank 25)\n",
      "* **Tim Tscharnke** (Rank 26)\n",
      "* **Philipp Haelg** (Rank 27)\n",
      "* **Ville Nousiainen** (Rank 28)\n",
      "* **Maciej Kreczmer** (Rank 29)\n",
      "* **Francesco de Fabiani** (Rank 30)\n",
      "* **Noah Hoffman** (Rank 31)\n",
      "* **Dietmar Nöckeler** (Rank 32)\n",
      "* **Nikolay Chebotko** (Rank 33)\n",
      "* **Yevgeniy Velichko** (Rank 34)\n",
      "* **Devon Kershaw** (Rank 35)\n",
      "* **Mattia Pellegrin** (Rank 36)\n",
      "* **Andrew Young** (Rank 37)\n",
      "* **Erik Bjornsen** (Rank 38)\n",
      "* **Ivan Babikov** (Rank 39)\n",
      "* **Aivar Rehemaa** (Rank 40)\n",
      "* **Veselin Tzinzov** (Rank 41)\n",
      "* **Algo Kärp** (Rank 42)\n",
      "* **Adrien Backscheider** (Rank 43)\n",
      "* **Andrew Musgrave** (Rank 44)\n",
      "* **Karel Tammjärv** (Rank 45)\n",
      "* **Sergey Mikayelyan** (Rank 46)\n",
      "* **Brian Gregg** (Rank 47)\n",
      "* **Fabio Pasini** (Rank 48)\n",
      "* **Alexander Lasutkin** (Rank 49)\n",
      "* **Imanol Rojo** (Rank 50)\n",
      "* **Peter Mlynár** (Rank 51)\n",
      "* **Kris Freeman** (Rank 52)\n",
      "* **Sergei Dolidovich** (Rank 53)\n",
      "* **Yerdos Akhmadiyev** (Rank 54)\n",
      "* **Sebastian Gazurek** (Rank 55)\n",
      "* **Cyril Miranda** (Rank 56)\n",
      "* **Max Hauke** (Rank 57)\n",
      "* **Andrew Musgrave** (Rank 58)\n",
      "* **Michail Semenov** (Rank 59)\n",
      "* **Edi Dadić** (Rank 60)\n",
      "* **Raido Ränkel** (Rank 61)\n",
      "* **Paul Constantin Pepene** (Rank 62)\n",
      "* **Javier Gutiérrez Cuevas** (Rank 63)\n",
      "* **Pawel Klisz** (Rank 64)\n",
      "* **Graeme Killick** (Rank 65)\n",
      "* **Maciej Staręga** (Rank 66)\n",
      "* **Callum Smith** (Rank 67)\n",
      "* **Hwang Jun-Ho** (Rank 68)\n",
      "* **Oleksii Krasovskyi** (Rank 69)\n",
      "* **Vytautas Strolia** (Rank 70)\n",
      "* **Sabahattin Oğlago** (Rank 71)\n",
      "* **Sun Qinghai** (Rank 72)\n",
      "* **Arvis Liepiņš** (Rank 73)\n",
      "* ** | Ground Truth: Iivo Niskanen|Daniel Richardsson|Johan Olsson|Dario Cologna\n",
      "ID: nt-234 | Response: 2013's match at Tianhe Stadium, Guangzhou | Ground Truth: Tianhe Stadium, Guangzhou\n",
      "ID: nt-236 | Response: Willie Robinson | Ground Truth: Willie Robinson\n",
      "ID: nt-239 | Response: 2010 | Ground Truth: 2010\n",
      "ID: nt-242 | Response: The Ae 4/4 weighs the least at 80 t. | Ground Truth: Re 4/4\n",
      "ID: nt-244 | Response: 2 | Ground Truth: 5\n",
      "ID: nt-253 | Response: 3 | Ground Truth: 3\n",
      "ID: nt-258 | Response: 2 | Ground Truth: 3\n",
      "ID: nt-268 | Response: 28 February 2012 | Ground Truth: 28 February 2012\n",
      "ID: nt-279 | Response:  | Ground Truth: 11\n",
      "ID: nt-285 | Response: Jean-Philippe Ruggia | Ground Truth: Jean-Philippe Ruggia\n",
      "ID: nt-286 | Response: The most common status for the trains is **Operational**. | Ground Truth: Operational\n",
      "Testcase nt-288 fails with error: Error tokenizing data. C error: Expected 6 fields in line 13, saw 7\n",
      "\n",
      "ID: nt-290 | Response: The Hispanic population with the greatest growth from 2000 to 2005 was **AIAN* ** with a growth rate of 33.56%. | Ground Truth: White\n",
      "ID: nt-296 | Response: 1993 World Championships | Ground Truth: World Championships\n",
      "ID: nt-301 | Response: 115 | Ground Truth: 115\n",
      "ID: nt-305 | Response: The provided table does not contain information about which country was the first to sell weapons to Iraq. | Ground Truth: Czechoslovakia\n",
      "ID: nt-306 | Response: Chicago won more. | Ground Truth: More\n",
      "ID: nt-309 | Response: 2005 European U23 Championships, Shot put | Ground Truth: European U23 Championships\n",
      "ID: nt-330 | Response: Ince Blundell, with a population of 518. | Ground Truth: Ince Blundell\n",
      "ID: nt-335 | Response: Bayern Munich and Borussia Dortmund have more wins than Werder Bremen. | Ground Truth: Borussia Dortmund|Bayern Munich\n",
      "ID: nt-341 | Response: 4 | Ground Truth: 3\n",
      "ID: nt-343 | Response: 1979 Asian Championships | Ground Truth: Asian Championships\n",
      "ID: nt-344 | Response: 317 | Ground Truth: 317\n",
      "ID: nt-354 | Response:  | Ground Truth: Less\n",
      "ID: nt-359 | Response: 2 | Ground Truth: 3\n",
      "ID: nt-360 | Response: 3 | Ground Truth: 3\n",
      "ID: nt-363 | Response: Silvia Sperber (FRG) | Ground Truth: Silvia Sperber (FRG)\n",
      "ID: nt-365 | Response: Thomas B. Hart | Ground Truth: Henry E. Prickett\n",
      "ID: nt-370 | Response: 1986 | Ground Truth: 1986\n",
      "ID: nt-380 | Response: Based on the provided data, Chipper Adams and Justin Beyendeza have **at least 2 wins**. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about this data! | Ground Truth: no\n",
      "ID: nt-388 | Response: 1 | Ground Truth: 1\n",
      "ID: nt-389 | Response: Well done (bien cuit) | Ground Truth: Over cooked (trop cuit, carbonisé\")'\n",
      "ID: nt-390 | Response: 62 years | Ground Truth: 62 years\n",
      "ID: nt-394 | Response: 7 years | Ground Truth: 7 years\n",
      "ID: nt-396 | Response: Valencia CC won the Division II Community College championship the most times, tied with **Chipola**. \n",
      "\n",
      "\n",
      "Let me know if you have any other questions about this data! | Ground Truth: Chipola\n",
      "ID: nt-405 | Response: 10 | Ground Truth: 7\n",
      "ID: nt-410 | Response:  | Ground Truth: Anime Friends\n",
      "ID: nt-414 | Response: Yes, Tripoli is still considered a municipality in Arcadia since the 2011 reformation. | Ground Truth: Yes\n",
      "ID: nt-417 | Response: 7 | Ground Truth: 7\n",
      "ID: nt-421 | Response: 6 | Ground Truth: 6\n",
      "ID: nt-430 | Response: 3 | Ground Truth: 2\n",
      "ID: nt-432 | Response: 203.0 km | Ground Truth: 203.0 km (126.1 mi)\n",
      "ID: nt-434 | Response: 7 | Ground Truth: 7\n",
      "ID: nt-441 | Response: 21 | Ground Truth: 21\n",
      "ID: nt-445 | Response: 2013 | Ground Truth: 2013\n",
      "ID: nt-450 | Response: 37 | Ground Truth: 33\n",
      "ID: nt-452 | Response: 3:40 | Ground Truth: 3:40\n",
      "ID: nt-463 | Response: 2,192 | Ground Truth: 2192\n",
      "ID: nt-464 | Response: Russia placed in first with the most earned medals. | Ground Truth: Russia\n",
      "ID: nt-472 | Response: NC State | Ground Truth: NC State\n",
      "ID: nt-486 | Response: 10,000 | Ground Truth: 9,997\n",
      "ID: nt-491 | Response: Cincinnati Saints | Ground Truth: Detroit Waza\n",
      "ID: nt-493 | Response: 162 | Ground Truth: 159\n",
      "ID: nt-494 | Response: 2 | Ground Truth: 2\n",
      "ID: nt-500 | Response: Ireland | Ground Truth: Ireland\n",
      "ID: nt-501 | Response: 6 | Ground Truth: 4\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for testcase_id in test_cases.index:\n",
    "    # need to check if processed\n",
    "    try:\n",
    "        df_path = wiki_tq_dir + test_cases.loc[testcase_id][\"context\"]\n",
    "        statement = test_cases.loc[testcase_id][\"utterance\"]\n",
    "        answer = test_cases.loc[testcase_id][\"targetValue\"]\n",
    "        df = pd.read_csv(df_path)\n",
    "        \n",
    "        prompt = build_prompt(df, statement)\n",
    "        response = query_groq(np.array([{\"role\": \"assistant\",\n",
    "                 \"content\": prompt}]))\n",
    "        \n",
    "        response = response.strip().strip(\"'\\\"\")\n",
    "        final_response = response.split(\"Answer:\")[-1].strip()\n",
    "        \n",
    "        print(f\"ID: {testcase_id} | Response: {final_response} | Ground Truth: {answer}\")\n",
    "        \n",
    "        # Save to results list\n",
    "        results.append({\n",
    "            \"ID\": testcase_id,\n",
    "            \"Response\": final_response,\n",
    "            \"Ground Truth\": answer\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Testcase {testcase_id} fails with error: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Response</th>\n",
       "      <th>Ground Truth</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nt-2</th>\n",
       "      <td>Wolfe Tones</td>\n",
       "      <td>Wolfe Tones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-9</th>\n",
       "      <td>Andri Aganits</td>\n",
       "      <td>Siim Ennemuist|Andri Aganits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-24</th>\n",
       "      <td>Sweden</td>\n",
       "      <td>Sweden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-36</th>\n",
       "      <td>Princeton</td>\n",
       "      <td>Princeton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nt-42</th>\n",
       "      <td>459,640</td>\n",
       "      <td>459,640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Response                  Ground Truth\n",
       "ID                                                \n",
       "nt-2     Wolfe Tones                   Wolfe Tones\n",
       "nt-9   Andri Aganits  Siim Ennemuist|Andri Aganits\n",
       "nt-24         Sweden                        Sweden\n",
       "nt-36      Princeton                     Princeton\n",
       "nt-42        459,640                       459,640"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_prompting = pd.DataFrame(results)\n",
    "results_df_prompting = results_df_prompting.set_index(\"ID\")\n",
    "results_df_prompting.to_csv(\"direct_prompting_results.csv\")\n",
    "results_df_prompting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: nt-9, Expected: siim ennemuist|andri aganits, Predicted: andri aganits \n",
      "ID: nt-43, Expected: seaford town, Predicted: east preston \n",
      "ID: nt-54, Expected: theodis tarver, Predicted: theodis tarver plays center \n",
      "ID: nt-72, Expected: 2, Predicted: 0 \n",
      "ID: nt-75, Expected: 18, Predicted: 10 \n",
      "ID: nt-80, Expected: less, Predicted: the dominican republic has won 12 \n",
      "ID: nt-81, Expected: chevrolet, Predicted: chevrolet has the most vehicles in the roster other than dodge \n",
      "ID: nt-85, Expected: george burley, Predicted: john wark \n",
      "ID: nt-86, Expected: serbian progressive party српска напредна странка  srpska napredna stranka, Predicted: the serbian progressive party has the most mps with 134 \n",
      "ID: nt-120, Expected: bahrain, Predicted: south korea has the most wins against bahrain \n",
      "ID: nt-138, Expected: switzerland, Predicted:  \n",
      "ID: nt-153, Expected: 10, Predicted: 100 \n",
      "ID: nt-154, Expected: pan troglodytes|nomascus leucogenys, Predicted: chimpanzee|gibbon \n",
      "ID: nt-171, Expected: 2, Predicted: 7 \n",
      "ID: nt-174, Expected: 26, Predicted: 30 \n",
      "ID: nt-182, Expected: 10, Predicted: 9 \n",
      "ID: nt-192, Expected: 4, Predicted: 5 \n",
      "ID: nt-201, Expected: park|missoula, Predicted: park|missoula have a larger combined population \n",
      "ID: nt-205, Expected: 5, Predicted: 50 \n",
      "ID: nt-229, Expected: tv sivaraopantulu, Predicted: tv sivaraopantulu held the position longer \n",
      "ID: nt-230, Expected: iivo niskanen|daniel richardsson|johan olsson|dario cologna, Predicted: here are the players who finished ahead of lukáš bauer based on the provided table\n",
      "\n",
      " dario cologna rank 3534434678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273 \n",
      "ID: nt-234, Expected: tianhe stadium guangzhou, Predicted: 2013 \n",
      "ID: nt-242, Expected: re 44, Predicted: the ae 4480 \n",
      "ID: nt-244, Expected: 5, Predicted: 2 \n",
      "ID: nt-258, Expected: 3, Predicted: 2 \n",
      "ID: nt-279, Expected: 11, Predicted:  \n",
      "ID: nt-286, Expected: operational, Predicted: the most common status for the trains is operational \n",
      "ID: nt-290, Expected: white, Predicted: the hispanic population with the greatest growth from 200020053356 \n",
      "ID: nt-296, Expected: world championships, Predicted: 1993 \n",
      "ID: nt-305, Expected: czechoslovakia, Predicted: the provided table does not contain information about which country was the first to sell weapons to iraq \n",
      "ID: nt-306, Expected: more, Predicted: chicago won more \n",
      "ID: nt-309, Expected: european u23, Predicted: 200523 \n",
      "ID: nt-330, Expected: ince blundell, Predicted: ince blundell with a population of 518 \n",
      "ID: nt-335, Expected: borussia dortmund|bayern munich, Predicted: bayern munich|borussia dortmund have more wins than werder bremen \n",
      "ID: nt-341, Expected: 3, Predicted: 4 \n",
      "ID: nt-343, Expected: asian championships, Predicted: 1979 \n",
      "ID: nt-354, Expected: less, Predicted:  \n",
      "ID: nt-359, Expected: 3, Predicted: 2 \n",
      "ID: nt-365, Expected: henry e prickett, Predicted: thomas b hart \n",
      "ID: nt-380, Expected: no, Predicted: based on the provided data chipper adams|justin beyendeza have at least 2 \n",
      "ID: nt-389, Expected: over cooked trop cuit carbonisé, Predicted: well done bien cuit \n",
      "ID: nt-396, Expected: chipola, Predicted: valencia cc won the division ii community college championship the most times tied with chipola \n",
      "\n",
      "\n",
      "let me know if you have any other questions about this data \n",
      "ID: nt-405, Expected: 7, Predicted: 10 \n",
      "ID: nt-410, Expected: anime friends, Predicted:  \n",
      "ID: nt-414, Expected: yes, Predicted: yes tripoli is still considered a municipality in arcadia since the 2011 \n",
      "ID: nt-430, Expected: 2, Predicted: 3 \n",
      "ID: nt-432, Expected: 20301261, Predicted: 2030 \n",
      "ID: nt-450, Expected: 33, Predicted: 37 \n",
      "ID: nt-464, Expected: russia, Predicted: russia placed in first with the most earned medals \n",
      "ID: nt-486, Expected: 9997, Predicted: 10000 \n",
      "ID: nt-491, Expected: detroit waza, Predicted: cincinnati saints \n",
      "ID: nt-493, Expected: 159, Predicted: 162 \n",
      "ID: nt-501, Expected: 4, Predicted: 6 \n",
      "----------------------------------------\n",
      "Accuracy: 46.46%\n"
     ]
    }
   ],
   "source": [
    "accuracy=0\n",
    "incorrect_ids=[]\n",
    "for index, row in results_df_prompting.iterrows():\n",
    "    y_pred= normalize_answer(row['Response'])\n",
    "    y_true= normalize_answer(row['Ground Truth'])\n",
    "    if y_pred == y_true:\n",
    "        accuracy+=1\n",
    "    else:\n",
    "        print(f\"ID: {index}, Expected: {y_true}, Predicted: {y_pred} \")\n",
    "        incorrect_ids.append(index)\n",
    "\n",
    "print(\"-\"*40)\n",
    "accuracy_percentage = (accuracy / results_df_prompting.shape[0]) * 100\n",
    "print(f\"Accuracy: {accuracy_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments: By verifying the failures, there are 9 more correct answers, dues to larger or different format of prompts: so the the accuracy using WikiTQ dataset with model gemma2, using chain of tables is ~55%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
